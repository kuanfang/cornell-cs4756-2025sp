{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN-bzlgeonZI"
      },
      "source": [
        "# **Assignment 5: Perception**\n",
        "\n",
        "### **Due Date**: 04/29/2025 at 11:59 PM\n",
        "\n",
        "### **Late Due Date**: 05/02/2025 at 11:59 PM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K4mmaQKo2SC"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "Welcome to Assignment 5 of CS 4756/5756. In this assignment, you will train policies under various state estimators. In Assignment 4, we tried to estimate the transition function through a world model. This assignment is very similar, but this time we will try estimating the state instead.\n",
        "\n",
        "This assignment is built up by the following components:\n",
        "\n",
        "- **[PROVIDED] Setup**: Dependency installing and initializations.\n",
        "- **[PROVIDED] Helper Functions**: Provided functions for visualization and evaluation.\n",
        "- **Part 1**: Train an expert PPO policy with StableBaselines3.\n",
        "- **Part 2**: Train a state estimator on RGB image data.\n",
        "- **Part 3**: Train a learner PPO policy using the state estimator.\n",
        "- **[GRAD] Part 4**: Combine the state estimator and policy into an end-to-end policy.\n",
        "\n",
        "You will use the **FetchReach-v4** environment for this assignment. Refer to the Gymnasium-Robotics website for more details about this [environment](https://robotics.farama.org/envs/fetch/reach/)\n",
        "\n",
        "Please read through the following paragraphs carefully.\n",
        "\n",
        "**Getting Started**: You should complete this assignment on [Google Colab](https://colab.research.google.com).\n",
        "\n",
        "**Evaluation**: Your code will be tested for correctness and, for certain assignments, speed. For this particular assignment, performance results will not be harshly graded (although we provide approximate expected reward numbers, you are not expected to replicate them exactly). Please remember that all assignments should be completed individually.\n",
        "\n",
        "**Academic Integrity**: We will be checking your code against other submissions in the class for logical redundancy. If you copy someone else's code and submit it with minor changes, we will know. These cheat detectors are quite hard to fool, so please don't try. We trust you all to submit your own work only; please don't let us down. If you do, we will pursue the strongest consequences available to us.\n",
        "\n",
        "**Getting Help**: The [Resources](https://www.cs.cornell.edu/courses/cs4756/2025sp/#resources) section on the course website is your friend! If you ever feel stuck in these projects, please feel free to avail yourself to office hours and Edstem! If you are unable to make any of the office hours listed, please let TAs know and we will be happy to assist. If you need a refresher for PyTorch, please see this [60 minute blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)! For Numpy, please see the quickstart [here](https://numpy.org/doc/stable/user/quickstart.html) and full API [here](https://numpy.org/doc/stable/reference/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5pI2Krtq5LO"
      },
      "source": [
        "# **[PROVIDED] Setup**\n",
        "\n",
        "Please run the cells below to install the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCnvkXR2q8Nv"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "USING_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if USING_COLAB:\n",
        "    !apt-get -qq update\n",
        "    !apt-get -qq install -y libosmesa6-dev libgl1-mesa-glx libglfw3 libgl1-mesa-dev libglew-dev patchelf\n",
        "    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "else:\n",
        "    !pip install torch torchvision torchaudio\n",
        "    !pip install numpy\n",
        "    !pip install tqdm\n",
        "    !pip install opencv-python\n",
        "\n",
        "!pip install matplotlib\n",
        "!pip install -U mediapy\n",
        "!pip install -U renderlab\n",
        "!pip install -U \"imageio<3.0\"\n",
        "!pip install stable_baselines3\n",
        "\n",
        "!git clone https://github.com/Farama-Foundation/Gymnasium-Robotics.git\n",
        "!pip install -e Gymnasium-Robotics\n",
        "sys.path.append('/content/Gymnasium-Robotics')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECUpHmbvq98v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Mujoco GLEW Setup\n",
        "try:\n",
        "    if _mujoco_run_once:  pass\n",
        "except NameError:\n",
        "    _mujoco_run_once = False\n",
        "\n",
        "if not _mujoco_run_once:\n",
        "    try:\n",
        "        os.environ['LD_PRELOAD']=os.environ['LD_PRELOAD'] + ':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
        "    except KeyError:\n",
        "        os.environ['LD_PRELOAD']='/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
        "\n",
        "    # Presetup so we don't see output on first env initialization\n",
        "    _mujoco_run_once = True\n",
        "    if USING_COLAB:\n",
        "        NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
        "        if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
        "            with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
        "                f.write(\"\"\"{\n",
        "                    \"file_format_version\" : \"1.0.0\",\n",
        "                    \"ICD\" : {\n",
        "                        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
        "                    }\n",
        "                }\"\"\")\n",
        "\n",
        "    # Set environment variable to support EGL (off-screen) rendering\n",
        "    %env MUJOCO_GL=egl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n-DR7UcrC-o"
      },
      "source": [
        "Please run the cells below to import necessary packages and set the initial seeding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfZnTFqNrGZF"
      },
      "outputs": [],
      "source": [
        "from gymnasium_robotics.envs.fetch.reach import MujocoFetchReachEnv, MujocoPyFetchReachEnv\n",
        "from torch.utils.data import DataLoader\n",
        "import gymnasium.wrappers as wrappers\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.distributions as D\n",
        "from tqdm import tqdm, trange\n",
        "import torch.optim as optim\n",
        "import gymnasium_robotics\n",
        "import gymnasium as gym\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E_pZopXrSBH"
      },
      "outputs": [],
      "source": [
        "seed = 695\n",
        "\n",
        "# Setting the seed to ensure reproducability\n",
        "def reseed(seed, env=None):\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    if env is not None:\n",
        "        env.unwrapped._np_random = gym.utils.seeding.np_random(seed)[0]\n",
        "\n",
        "reseed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDaQ6YBJraLN"
      },
      "source": [
        "Please run the cells below to simplify the `FetchReach-v4` environment for easier training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uluYMA4VrdgR"
      },
      "outputs": [],
      "source": [
        "# In this block we define wrappers necessary to simplify the environment MDP\n",
        "def wrap_reach_fixed_goal(env, fixed_goal_noise : float = 0.0):\n",
        "    g = np.array([1.486, 0.73, 0.681], dtype=np.float32)\n",
        "    def sample_goal():\n",
        "      noise = np.random.uniform(-fixed_goal_noise, fixed_goal_noise, size=3)\n",
        "      return g + noise\n",
        "    env.unwrapped._sample_goal = sample_goal\n",
        "    return env\n",
        "\n",
        "class FetchRewardWrapper(gym.Wrapper):\n",
        "    def reset(self, *args, **kwargs):\n",
        "        obs, info = self.env.reset(*args, **kwargs)\n",
        "        self.prev_dist = np.linalg.norm(obs['achieved_goal'] - obs['desired_goal'])\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action) # Terminated is never set to true\n",
        "        current_dist = np.linalg.norm(obs['achieved_goal'] - obs['desired_goal'])\n",
        "        reward = (self.prev_dist - current_dist) * 10\n",
        "        self.prev_dist = current_dist\n",
        "        return obs, reward, info['is_success'], truncated, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zt69CANBsFNl"
      },
      "outputs": [],
      "source": [
        "# Let's initialize the environment first\n",
        "reseed(seed)\n",
        "\n",
        "def make_fetch_env(width : int = 120, height : int = 120, fixed_goal_noise : float = 0.03):\n",
        "    env = MujocoFetchReachEnv(width=width, height=height, render_mode=\"rgb_array\")\n",
        "    env = wrappers.TimeLimit(env, 50)\n",
        "    env = wrap_reach_fixed_goal(env, fixed_goal_noise)\n",
        "    env = FetchRewardWrapper(env)\n",
        "    env = wrappers.FilterObservation(env, [\"desired_goal\", \"observation\"])\n",
        "    env = wrappers.FlattenObservation(env)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjkxLxr-sx9n"
      },
      "source": [
        "# **[PROVIDED] Helper Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hShr6ALOtasD"
      },
      "source": [
        "### **Policy Evaluation Functions**\n",
        "\n",
        "The `evaluate_policy` function takes a policy actor, an environment whose output observations can be applied to the actor, and evaluates the policy by doing the following:\n",
        "\n",
        "- Rollout actor for a default of 100 trajectories, and record the total reward.\n",
        "- Return the average trajectory rewards over these episodes.\n",
        "\n",
        "**Note:** Since the actor we will be defining in this assignment exclusively uses a StableBaselines3 PPO policy, then the environment provided must be an instance of `VecEnv`, more information introduced in Part 1.\n",
        "\n",
        "The `success_rate` function is similar to the `evaluate_policy` function except that it takes a regular gymnasium environment instead of a vectorized environment. It also records the success as a percentage instead of the total reward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZob-Csztqf9"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(actor, environment, num_episodes=100, progress=True):\n",
        "    \"\"\"\n",
        "        Returns the mean trajectory reward of rolling out `actor` on `environment.\n",
        "\n",
        "        Parameters\n",
        "        - actor: PPOActor instance, defined in Part 1.\n",
        "        - environment: classstable_baselines3.common.vec_env.VecEnv instance.\n",
        "        - num_episodes: Total number of trajectories to collect and average over.\n",
        "    \"\"\"\n",
        "\n",
        "    total_rew = 0\n",
        "    iterate = (trange(num_episodes) if progress else range(num_episodes))\n",
        "\n",
        "    for _ in iterate:\n",
        "        obs = environment.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = actor.select_action(obs)\n",
        "            next_obs, reward, done, info = environment.step(action)\n",
        "            total_rew += reward\n",
        "            obs = next_obs\n",
        "\n",
        "    return (total_rew / num_episodes).item()\n",
        "\n",
        "\n",
        "def success_rate(actor, environment, num_episodes=100, progress=True):\n",
        "    \"\"\"\n",
        "        Returns the percentage of successful trajectories of `actor` on `environment`.\n",
        "\n",
        "        Parameters\n",
        "        - actor: PPOActor instance, defined in Part 1.\n",
        "        - environment: Gymnasium environment.\n",
        "        - num_episodes: Total number of trajectories to collect and average over.\n",
        "    \"\"\"\n",
        "\n",
        "    total_success = 0\n",
        "    iterate = (trange(num_episodes) if progress else range(num_episodes))\n",
        "\n",
        "    for _ in iterate:\n",
        "        obs, info = environment.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = actor.select_action(obs)\n",
        "            next_obs, reward, done, truncated, info = environment.step(action)\n",
        "            obs = next_obs\n",
        "\n",
        "            if done: total_success += 1\n",
        "            if truncated: break\n",
        "\n",
        "    return (total_success / num_episodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FDcaRmlt4fF"
      },
      "source": [
        "### **Notes About Fetch Reach Environment**\n",
        "\n",
        "The environment uses a Fetch Robot, which is a 7-DoF Mobile Manipulator.\n",
        "\n",
        "The task is a _goal-reaching task_: The observation space contains `observation` which includes the state of the robot in the environment, and `desired_goal` which specifies the xyz coordinate that the robot's gripper aims to reach.\n",
        "\n",
        "See https://robotics.farama.org/envs/fetch/reach/ for more details.\n",
        "\n",
        "If the goal is reached, `info['is_success']` will be set to 1, and this is an indication that we should terminate the rollout.\n",
        "\n",
        "The reward is normally -1 per timestep spent in the environment without completing the task, with 50 steps being the limit (so -50 is the worst episode return).\n",
        "\n",
        "> Note: For this assignment, we've modified the environment so that it only has a partially random fixed goal to reach, has better reward shaping, and renders smaller frames. This is to make training easier and quicker later on.\n",
        "\n",
        "**Run the cell below to create the environment:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q6JuwhxuH12"
      },
      "outputs": [],
      "source": [
        "real_env = make_fetch_env()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIBlD6aKwNvW"
      },
      "source": [
        "# **Part 1: Train Expert Using StableBaselines3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehj3Azx9wRCl"
      },
      "source": [
        "### **1.1: [PROVIDED] Introduction To Stable Baselines 3**\n",
        "\n",
        "StableBaselines3 is popular off-the-shelf set of reliable implementations of reinforcement learning algorithms in PyTorch. In this assignment, we will be using its PPO (Proximal Policy Gradient) implementation as our policy.\n",
        "\n",
        "Each algorithm implementation is a subclass of the `stable_baselines3.common.base_class.BaseAlgorithm` class, which provides us with the following functions:\n",
        "\n",
        "- `learn(total_timesteps, callback=None, log_interval=100, tb_log_name='run', reset_num_timesteps=True, progress_bar=False)`\n",
        "  - This is the training loop of any of the RL algorithm implementations. Training is done by calling this function with an appropriate amount of `total_timesteps`.\n",
        "- `predict(observation)`\n",
        "  - Returns a tuple `(predicted_action, next_hidden_state)` based on input `observation`. If we are not using an RNN, the next hidden state can be neglected.\n",
        "- `save(path)`\n",
        "  - Saves the current policy parameters into a `.zip` file with given `path`. Note that the `path` does not have the `.zip` postfix.\n",
        "- `load(path, env=None)`\n",
        "  - Loads a saved a `.zip` checkpoint into this RL implementation model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEJ2cw7GwlxN"
      },
      "source": [
        "### **1.2: [PROVIDED] Hyperparameters**\n",
        "\n",
        "The implementation has a set of hyperparameters that can be tuned towards better performance. For the sake of simplicity, we will provide the hyperparameters for the StableBaselines3 PPO implementation. The main ones we specify include the following:\n",
        "\n",
        "- `n_steps`: the number of steps to run with the environment for each update to the policy network.\n",
        "- `net_arch`: The network architecture of the policy network and the critic network:\n",
        "  - `pi`: a list that specifies the hidden dimensions of the policy network. The input and output dimension are determined by the environment associated with this policy.\n",
        "  - `vf`: a list that specifies the hidden dimensions of the critic network.\n",
        "  - `activation_fn`: Nonlinearity to be applied between each of the MLP layers.\n",
        "\n",
        "For a more comprehensive list and description of each of these hyperparameters, visit the official [documentation page](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters) for more information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXjygzu_wprZ"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.vec_env.base_vec_env import VecEnv\n",
        "\n",
        "hyperparameters = {\n",
        "    \"n_steps\": 512,\n",
        "    \"policy_kwargs\": {\n",
        "        \"net_arch\": {\n",
        "            \"pi\": [128],\n",
        "            \"vf\": [128],\n",
        "            \"activation_fn\": \"tanh\",\n",
        "        }\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGXO6sL0w1_P"
      },
      "source": [
        "### **1.3: Vectorized Environmnent**\n",
        "\n",
        "For any StableBaselines3 algorithm implementation, the gymnasium environment used need to be converted into a [vectorized environment](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#) of `VecEnv` type.\n",
        "\n",
        "A vectorized environment stacks multiple independent environments into one, stepping multiple `n` environments each time. If we set the the `n_envs` parameter to 3, then 3 environments will be stepped each time the VecEnv is stepped.\n",
        "\n",
        "**For the rest of this assignment, all vectorized environments with `n_env=n` will be described as n-vectorized.**\n",
        "\n",
        "With a vectorized environment that steps multiple environments at the same time, the model learning process can be made more efficient through parallelization trajectory collection across these independent environments. This `n_envs` parameter can be tailored to the specific machines.  \n",
        "\n",
        "**Important Differences:**\n",
        "- The vectorized environments now require input action to be a shape of `n_envs * act_dim`. The output observation from `step` and `reset` will also have the shape of `n_envs * obs_dim`.\n",
        "- The VecEnv `reset()` function returns only the observation, while the gymnasium.Env `reset()` function returns a tuple `(observation, info_dict)`.\n",
        "- The `vec_env.step(action)` function returns a 4-tuple of `(obs, reward, terminated, info)`, while the `gym_env.step(action)` returns a 5-tuple of `(obs, reward, terminated, truncated, info)`. The `terminated` value from VecEnv would equivalent to the gymnasium environment's `terminated or truncated`.\n",
        "\n",
        "\n",
        "A VecEnv instance can be created using the `make_vec_env` function, which takes the id of the wanted gymnasium environment, as well as the number of environments needed. This function has the following key parameters\n",
        "- `env_id`: the id of the gymnasium environment, or instantiated gym environment, or a callable that returns an env.\n",
        "- `n_envs`: The number of environments to have in parallel.\n",
        "- `seed`: The initial seed for the random number generator.\n",
        "- `env_kwargs`: An optional parameter to pass into the environment constructor.\n",
        "\n",
        "More detailed function documentation can be found in this [page](https://stable-baselines3.readthedocs.io/en/master/common/env_util.html#stable_baselines3.common.env_util.make_vec_env).\n",
        "\n",
        "**Instructions** For this part, please create two vectorized version of `FetchReach-v4` with 3 and 1 environments stacked. Note that because of our wrappers, you need to pass a callable, we have one called `make_fetch_env` defined above. During the initialization of the vectorized environments, leave the parameters of `make_fetch_env` their default values (you can directly pass in the function as the environment constructor for `make_vec_env`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMav4u5zw6un"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "# TODO: Instantiate\n",
        "real_vec_env_3 = None\n",
        "real_vec_env_1 = None\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdEW3zSuxFG8"
      },
      "source": [
        "### **1.4: Actor Definition**\n",
        "\n",
        "**Instruction**: You will need to implement the following PPOActor class, which serves as a wrapper to provide PPO model predictions.\n",
        "- `__init__`: Takes a path to the checkpoint and the corresponding environment, and load an instance of this PPO checkpoint. However if a PPO model is given, then the internally representing model uses that directly instead. This is for use in the Callback function, and since we provide that implementation for you, you will only need to implement the model loading portion of the constructor.\n",
        "- `select_action`: Takes an observation and produce the corresponding action prediction from the checkpoint PPO model using **deterministic** mode. While implementing, take note of the output of the `predict` function.\n",
        "\n",
        "**Note:** To try and limit the noise introduced in the environment and make it easier to achieve expected ranges, please use the parameter `deterministic=True` in your `section_action` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzO6oFFTxZkU"
      },
      "outputs": [],
      "source": [
        "class PPOActor():\n",
        "    def __init__(self, ckpt: str=None, environment: VecEnv=None, model : PPO =None):\n",
        "        '''\n",
        "          Requires environment to be a 1-vectorized environment\n",
        "\n",
        "          The `ckpt` is a .zip file path that leads to the checkpoint you want\n",
        "          to use for this particular actor.\n",
        "\n",
        "          If the `model` variable is provided, then this constructor will store\n",
        "          that as the internal representing model instead of loading one from the\n",
        "          checkpoint path\n",
        "        '''\n",
        "        assert ckpt is not None or model is not None\n",
        "\n",
        "        if model is not None:\n",
        "            self.model = model\n",
        "            return\n",
        "\n",
        "        # TODO: Load checkpoint into a `PPO` class\n",
        "        self.model = None\n",
        "        # END TODO\n",
        "\n",
        "    def select_action(self, obs):\n",
        "        '''Gives the action prediction of this particular actor'''\n",
        "\n",
        "        # TODO: compute action of the PPO policy using deterministic mode\n",
        "\n",
        "        # END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F3ibHjrxpHH"
      },
      "source": [
        "### **1.5: [PROVIDED] Callbacks**\n",
        "\n",
        "To visualize the training process, since it could take a significant amount of time, StableBaselines3 provides a mean for us to visualize the training progress through a `BaseCallback` class instance, which can be optionally passed in as a parameter of the `learn` function. This Callback function is customizable by defining a subclass of `BaseCallback`.\n",
        "\n",
        "For this part, we provide you with a customized callback that evaluates the model under training every 256 steps on an evaluating environment, which will be the 1-vectorized environment you have instantiated in the previous portion. Based on this evaluation result, this callback will save a checkpoint of the model if it is, so far, the best performing model. At the end of training, a plot of all evaluation results with respect to number of steps will be generated.\n",
        "\n",
        "You are free to modify this callback class to help you visualize training in any way most convenient for you, but is **NOT REQUIRED**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glP9YElexuWh"
      },
      "outputs": [],
      "source": [
        "class PPOCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0, save_path='default', eval_env=None, graph=True):\n",
        "        super(PPOCallback, self).__init__(verbose)\n",
        "        self.rewards = []\n",
        "        self.graph = graph\n",
        "\n",
        "        self.save_freq = 256\n",
        "        self.min_reward = -np.inf\n",
        "        self.actor = None\n",
        "        self.eval_env = eval_env\n",
        "\n",
        "        self.save_path = save_path\n",
        "        self.eval_steps = []\n",
        "        self.eval_rewards = []\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        \"\"\"\n",
        "        This method is called before the first rollout starts.\n",
        "        \"\"\"\n",
        "        self.actor = PPOActor(model=self.model)\n",
        "\n",
        "    def _on_rollout_start(self) -> None:\n",
        "        \"\"\"\n",
        "        A rollout is the collection of environment interaction\n",
        "        using the current policy.\n",
        "        This event is triggered before collecting new samples.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _on_rollout_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before updating the policy.\n",
        "        \"\"\"\n",
        "        episode_info = self.model.ep_info_buffer\n",
        "        rewards = [ep_info['r'] for ep_info in episode_info]\n",
        "        mean_rewards = np.mean(rewards)\n",
        "        self.rewards.append(mean_rewards)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        \"\"\"\n",
        "        This method will be called by the model after each call to `env.step()`.\n",
        "\n",
        "        For child callback (of an `EventCallback`), this will be called\n",
        "        when the event is triggered.\n",
        "\n",
        "        :return: If the callback returns False, training is aborted early.\n",
        "        \"\"\"\n",
        "        if self.eval_env is None:\n",
        "            return True\n",
        "\n",
        "        if self.num_timesteps % self.save_freq == 0 and self.num_timesteps != 0:\n",
        "            mean_reward = evaluate_policy(self.actor, environment=self.eval_env, num_episodes=20)\n",
        "            print(f'evaluating {self.num_timesteps=}, {mean_reward=}=======')\n",
        "\n",
        "            self.eval_steps.append(self.num_timesteps)\n",
        "            self.eval_rewards.append(mean_reward)\n",
        "            if mean_reward > self.min_reward:\n",
        "                self.min_reward = mean_reward\n",
        "                self.model.save(self.save_path)\n",
        "                print(f'model saved on eval reward: {self.min_reward}')\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _on_training_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before exiting the `learn()` method.\n",
        "        \"\"\"\n",
        "        print(f'model saved on eval reward: {self.min_reward}')\n",
        "\n",
        "        if self.graph:\n",
        "            plt.plot(self.eval_steps, self.eval_rewards, c='red')\n",
        "            plt.xlabel('Episodes')\n",
        "            plt.ylabel('Rewards')\n",
        "            plt.title('Rewards over Episodes')\n",
        "\n",
        "            plt.show()\n",
        "            plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoHN_skdx_Px"
      },
      "source": [
        "### **1.6 PPOActor Initialization And Training**\n",
        "\n",
        "The `stable_baselines3.ppo.PPO` class inherits from the `BaseAlgorithm` class described at the beginning of this section, and is specifically implemented for the PPO algorithm. To initialize a class, the following parameters are especially important:\n",
        "- `policy: str`: The policy type we use to train the policy, common ones include MlpPolicy and CnnPolicy. In our case, we will be using the MlpPolicy.\n",
        "- `env: VecEnv`: The environment that the policy rollouts on for training, must be vectorized or it will be vectorized by the PPO implementation\n",
        "- `n_steps`: number of steps to optimize the policy for\n",
        "- `device`: The device to put the model on (For this assignment, if you're not able to reach the performance bounds, try setting this parameter to cpu)\n",
        "- Other hyperparameters specified in the `hyperparameters` dictionary we provided, can be directly applied using the `**` operator.\n",
        "\n",
        "**Instructions**\n",
        "- Initialize a PPO MLP policy as expert, using the 3-env VecEnv initialized in the previous part and pass in the given hyperparameters.\n",
        "- Train the expert with an instance of the `PPOCallback` defined before. No need to save the resulting model into checkpoint since that is done for you in the Callback class\n",
        "  - (HINT): Look at the beginning of Part 1 for useful functions for training.\n",
        "\n",
        "\n",
        "**Estimated Training Time**:\n",
        "- 2 - 4 minutes on Google Colab CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ve9Hy7szyFqM"
      },
      "outputs": [],
      "source": [
        "reseed(seed)\n",
        "ckpt_path = 'expert'\n",
        "total_steps = 20000\n",
        "expert_callback = PPOCallback(save_path=ckpt_path, eval_env=real_vec_env_1)\n",
        "\n",
        "# TODO: Instantiate and train\n",
        "expert = None\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5Dxvs79yRA_"
      },
      "source": [
        "### **1.7: Evaluate Expert**\n",
        "\n",
        "**Instructions** Initialize an expert PPOActor instance from the checkpoint and evaluate the expert policy for 100 trajectories using the `evaluate_policy` and `success_rate` function on the real environment.\n",
        "\n",
        "**Expected Reward**: Around 1.5 - 1.7 on `real_vec_env_1`\n",
        "\n",
        "**Expected Success**: Around 0.95 on `real_env`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DRa-zrNyUWF"
      },
      "outputs": [],
      "source": [
        "expert = PPOActor(ckpt_path, real_vec_env_1)\n",
        "\n",
        "# TODO: Evaluate\n",
        "\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fDBniVPy4i7"
      },
      "source": [
        "# **Part 2: Collect Data And Train State Estimator**\n",
        "\n",
        "### **2.1: [PROVIDED] Overview**\n",
        "Unlike in simulation, we might not obtain the exact state in real world scenarios, and instead have to estimate it. We emulate that property in this assignment here.\n",
        "\n",
        "Assuming we do not have the underlying logic to the `FetchReach-v4`, given that we have an expert policy in solving this particular problem, we take the following state estimation approach to learn an RL policy that can be applied to the real scenario.\n",
        "\n",
        "In real life, we might not have such a trained expert, and human operating the robot remotely could be one source of expert data.\n",
        "\n",
        "1. Rollout a series of expert trajectories in the true environment (analogous to collecting a set of human demonstrations on the robot).\n",
        "    - Note that we need to record both perception data and the true state.\n",
        "2. Define and train a state estimator with the perception data as input.\n",
        "3. Define a new environment that applies the trained state estimator\n",
        "4. Learn an RL policy under the learned environment\n",
        "5. Evaluate this policy using the real environment\n",
        "\n",
        "You will need to implement the following functions and classes\n",
        "- `data_collect`: a helper function that rolls out a policy on an environment, and returning a tuple of lists representing the transitions\n",
        "- `StateEstimator` : a `torch.nn` module defining the architecture for perception.\n",
        "- `train_state_estimator` and `eval_state_estimator`: Training and evaluation loop of the state estimator\n",
        "\n",
        "Follow the instructions below to implement each of these components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-yb2VBc1AIX"
      },
      "source": [
        "### **2.2: Collect Data**\n",
        "\n",
        "**Instructions**\n",
        "\n",
        "The `data_collect` function should rollout a policy actor on the environment for a total of `num_steps`, with a maximum trajectory length of `traj_max_length`, then returning 2 lists: `renders` and `observations`, where `renders ` is the camera view at every step, and `observations` is the state at every step.\n",
        "\n",
        "**Hints:**\n",
        "- The `.render()` function for MuJoCo environments will be useful.\n",
        "- Also remember to apply the given transform to all obtained renders.\n",
        "- Make sure to end a trajectory if `done` or `truncated` is true."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Q1PpKJD1C1k"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as T\n",
        "\n",
        "def data_collect(num_steps: int, traj_max_length: int, data_env: gym.Env, actor: PPOActor = None):\n",
        "    renders, observations = [], []\n",
        "    transform = T.Compose([T.ToPILImage(), T.ToTensor()])\n",
        "\n",
        "    # TODO: Step and collect data\n",
        "\n",
        "    # END TODO\n",
        "\n",
        "    return renders, observations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXjJVnQB3o02"
      },
      "source": [
        "**Instructions**\n",
        "Run data collection function on the real environment with the expert policy trained in part 1.\n",
        "\n",
        "**Note**: The `data_collect` function requires the environment provided to be a regular gymnasium environment instead of a vectorized environment. Please make sure to not confuse it with `real_vec_env_1` defined in part 1.3.\n",
        "\n",
        "**Note**: Here is a list of currently created environments:\n",
        "- `real_env`\n",
        "- `real_vec_env_1`\n",
        "- `real_vec_env_3`\n",
        "\n",
        "Refer to function documentation for selecting which one to use when doing function calls.\n",
        "\n",
        "**Estimated Collection Time**:\n",
        "- 2 - 4 minutes on Google Colab CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR00Niae3z3r"
      },
      "outputs": [],
      "source": [
        "total_steps = 10000\n",
        "traj_max_length = 100\n",
        "reseed(seed, env=real_env)\n",
        "\n",
        "# TODO: Collect data\n",
        "renders, observations = None, None\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL0bQxUv438q"
      },
      "source": [
        "### **2.3: [PROVIDED] Visualize And Create Dataset**\n",
        "\n",
        "**Note** The below visualization is showing multiple coordinates of the observation at the same time, so it looks a bit weird. You should see 3 distinct curves that oscillate up and down."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGDhQYfS48Lh"
      },
      "outputs": [],
      "source": [
        "def visualize_collected_data(renders, observations):\n",
        "    '''\n",
        "        Takes the first 300 data points and generates a plot of the observations and next_obs.\n",
        "    '''\n",
        "    print(f'Dataset Size: {len(observations)}')\n",
        "    print(f'Observation Size: {observations[0].shape}')\n",
        "    print(f'Render Size: {renders[0].shape}')\n",
        "    plt.close()\n",
        "    plt.plot(np.arange(300), [obs[3:6] for obs in observations[:300]], c='blue')\n",
        "    plt.show()\n",
        "\n",
        "visualize_collected_data(renders, observations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a26e_3s65fUE"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class RenderDataset(Dataset):\n",
        "    def __init__(self, renders, observations):\n",
        "        self.renders = renders\n",
        "        self.observations = [obs.astype(np.float32) for obs in observations]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.observations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'render': self.renders[idx],\n",
        "            'observation': self.observations[idx]\n",
        "        }\n",
        "\n",
        "split = len(observations) // 5\n",
        "val_data = RenderDataset(renders[:split], observations[:split])\n",
        "train_data = RenderDataset(renders[split:], observations[split:])\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=128)\n",
        "val_dataloader = DataLoader(val_data, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8BLeYPu5pur"
      },
      "source": [
        "### **2.4: Define State Estimator**\n",
        "\n",
        "The `StateEstimator` class should define a neural network that takes a render and outputs the corresponding state in the state space. The network should have the following architecture:\n",
        "\n",
        "- Layer 1: a conv block made from a `nn.Sequential` module.\n",
        "    - There should be three convolutional layers that go from `3 -> 8 -> 16 -> 32` channels.\n",
        "    - Each convolutional layer should have a `kernel_size=3`, `stride=2`, and `padding=1`.\n",
        "    - There should be a `nn.ReLU` after every convolutional layer.\n",
        "    - The final layer in the block should be a `nn.AdaptiveAvgPool2d` that reduces both the height and width of the image to 1.\n",
        "- Layer 2: a fully-connected layer with `32` input nodes and `16` output nodes, followed by another ReLU.\n",
        "- Output layer: a fully-connected layer with `16` input nodes and `13` output nodes.\n",
        "\n",
        "The `forward` function should take only one input: `renders` of size `b x 3 x 120 x 120`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da7bPGKr8dig"
      },
      "outputs": [],
      "source": [
        "class StateEstimator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StateEstimator, self).__init__()\n",
        "\n",
        "        # TODO: Define architecture\n",
        "        self.conv = None\n",
        "        self.fc1 = None\n",
        "        self.fc2 = None\n",
        "        # END TODO\n",
        "\n",
        "    def forward(self, renders):\n",
        "        # TODO: Calculate the estimated state\n",
        "        return None\n",
        "        # END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2NELEGCAtu6"
      },
      "source": [
        "### **2.5: Training And Validation Function For State Estimator**\n",
        "\n",
        "The `train_state_estimator` function should train the provided model for one epoch, using the optimizer and criterion provided on the given train_dataloader. This function should iterate through each batch of the `train_dataloader` once, update the state estimator based on the loss calculated by criterion, then step the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_acWEzVA8CA"
      },
      "outputs": [],
      "source": [
        "def train_state_estimator(model, optimizer, criterion, train_dataloader):\n",
        "    '''\n",
        "        This function should train the torch model `model` using the\n",
        "        optim `optimizer` and `criterion` as loss function, on one pass\n",
        "        of the `train_dataloader`\n",
        "\n",
        "        This is should train the model for on epoch, as in one pass through\n",
        "        the training data.\n",
        "\n",
        "        Returns: the mean criterion loss across each batch of the dataset.\n",
        "    '''\n",
        "    total_loss, cnt = 0, 0\n",
        "    model.train()\n",
        "\n",
        "    # TODO: Update the model for one epoch\n",
        "\n",
        "    # END TODO\n",
        "\n",
        "    return total_loss / cnt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMeGk1-ZBGg3"
      },
      "source": [
        "The `eval_state_estimator` function is similar to the `train_state_estimator` function with iteration through the batches of the `eval_dataloader` and computes the loss using the given criterion. Note that no update to model should be made and gradients should not be calculated during the forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzlMGP2ABtT8"
      },
      "outputs": [],
      "source": [
        "def eval_state_estimator(model, criterion, eval_dataloader):\n",
        "    '''\n",
        "        This function should evaluate the torch model `model` using\n",
        "        `criterion` as loss function, on one pass of the `eval_dataloader`\n",
        "\n",
        "        This is should evaluate the model on the validation dataset.\n",
        "\n",
        "        Take note that during evaluation, the model should not be updated\n",
        "        in any way and gradients should not be calculated.\n",
        "\n",
        "        Returns: the mean criterion loss across each batch of the dataset.\n",
        "\n",
        "    '''\n",
        "    total_loss, cnt = 0, 0\n",
        "    model.eval()\n",
        "\n",
        "    # TODO: Evaluate the model across the whole dataset\n",
        "\n",
        "    # END TODO\n",
        "\n",
        "    return total_loss / cnt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tDXbUGMB1o_"
      },
      "source": [
        "### **2.6: Train The State Estimator**\n",
        "\n",
        "Train an instance of `StateEstimator` for `20` epochs with the dataloader built in previous section, using Adam optimizer and MSE loss, with an `lr` of `0.0001`. Provide a plot of training and evaluation losses with respect to training epochs, and also print out the final evaluation loss.\n",
        "\n",
        "**Estimated Training Time**:\n",
        "- 2 - 4 minutes on Google Colab CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efewKZofB9Jt"
      },
      "outputs": [],
      "source": [
        "num_epochs = 20\n",
        "reseed(seed)\n",
        "lr = 0.0001\n",
        "\n",
        "state_estimator = StateEstimator()\n",
        "optimizer = torch.optim.Adam(state_estimator.parameters(), lr=lr)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# TODO: Train and evaluate state estimator\n",
        "train_losses, eval_losses = [], []\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz2_YAZ9CKYR"
      },
      "source": [
        "### **2.7: [PROVIDED] Build Gym Environment With State Estimator**\n",
        "\n",
        "The following `StateEstimatorEnv` class is largely defined for you to train your next PPO policy. In this environment, the reward is calculated the same as `real_env`.\n",
        "\n",
        "To initialize a `StateEstimatorEnv` environment, a `state_estimator` (an instance of StateEstimator in this case) should be passed in as argument, which will be used in the `step()` and `reset()` functions.\n",
        "\n",
        "This environment is registered with an id of **StateEstimatorFetch**, which can be initialized using `gym.make` or directly initializing it.\n",
        "\n",
        "**Run the following cell to define and register this environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1hkXNfuCo3j"
      },
      "outputs": [],
      "source": [
        "class StateEstimatorEnv(gym.Env):\n",
        "    def __init__(self, estimator: StateEstimator, render_mode: str='rgb_array'):\n",
        "        super(StateEstimatorEnv, self).__init__()\n",
        "        self.metadata = { 'render_modes': ['human', 'rgb_array'], 'render_fps': 30 }\n",
        "        self.render_mode = 'rgb_array'\n",
        "        self.estimator = estimator\n",
        "        self.corr_env = real_env\n",
        "\n",
        "        self.transform = T.Compose([T.ToPILImage(), T.ToTensor()])\n",
        "        self.observation_space = self.corr_env.observation_space\n",
        "        self.action_space = self.corr_env.action_space\n",
        "        self.obs_min = torch.tensor(self.corr_env.observation_space.low)\n",
        "        self.obs_max = torch.tensor(self.corr_env.observation_space.high)\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        pass\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        obs, info = self.corr_env.reset()\n",
        "        rgb_array = self.corr_env.render()\n",
        "\n",
        "        transformed = self.transform(rgb_array).unsqueeze(0)\n",
        "        estimate = self.estimator(transformed)\n",
        "        clipped = torch.clamp(estimate, self.obs_min, self.obs_max)\n",
        "        return clipped.detach().numpy(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.corr_env.step(action)\n",
        "        rgb_array = self.corr_env.render()\n",
        "\n",
        "        transformed = self.transform(rgb_array).unsqueeze(0)\n",
        "        estimate = self.estimator(transformed)\n",
        "        clipped = torch.clamp(estimate, self.obs_min, self.obs_max)\n",
        "        return clipped.detach().numpy(), reward, bool(terminated), truncated, {}\n",
        "\n",
        "gym.register(id='StateEstimatorFetch', entry_point=StateEstimatorEnv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ6MsUIfCxwl"
      },
      "source": [
        "# **Part 3: Train Policy Using State Estimator**\n",
        "\n",
        "Now that we have learned a model that estimates the state of the real environment, it's time to train an policy on this model. In this part, you will learn and evaluate a PPO policy on the learned state estimator, similar to what happened in Part 1, using functions defined in the Helper Function section, Part 1, and Part 2.\n",
        "\n",
        "**Follow instructions to complete each component**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oeTupkCDGXH"
      },
      "source": [
        "### **3.1: Train New PPO On State Estimator**\n",
        "\n",
        "**Instruction 1** Initialize a 3-vectorized and a 1-vectorized `StateEstimatorFetch` environment.\n",
        "\n",
        "**Instruction 2** For this part, we will train a separate PPO policy using the learned model environment. This model should be trained with the state estimator learned in the previous part for 10000 steps, under a 3-vectorized environment, using the same hyperparameters provided in Part 1.\n",
        "\n",
        "**Note**: Here is a list of created environments after running this following cell:\n",
        "- `real_env`\n",
        "- `real_vec_env_1`\n",
        "- `real_vec_env_3`\n",
        "- `state_vec_env_1`\n",
        "- `state_vec_env_3`\n",
        "\n",
        "Refer to function documentation for selecting which one to use when doing function calls.\n",
        "\n",
        "**Estimated Training Time**:\n",
        "- 6 - 10 minutes on Google Colab CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0FjmfniDX9j"
      },
      "outputs": [],
      "source": [
        "learner_ckpt_path = 'learner'\n",
        "total_steps = 10000\n",
        "reseed(seed)\n",
        "\n",
        "# TODO 1: Create vectorized state estimator environments (HINT: use env_kwargs)\n",
        "env_kwargs = None\n",
        "state_vec_env_1 = None\n",
        "state_vec_env_3 = None\n",
        "# END TODO\n",
        "\n",
        "learner_callback = PPOCallback(save_path=learner_ckpt_path, eval_env=state_vec_env_1)\n",
        "\n",
        "# TODO 2: Initiate training\n",
        "learner = None\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3Fq_DZqD9Kh"
      },
      "source": [
        "### **3.2: Evaluate Learned Policy**\n",
        "\n",
        "Evaluate the learner policy on both the learned state estimator environment and the real environment. Also print out the success rate on the real environment. Note to save time only use 20 trajectores for `state_vec_env_1` and 100 trajectories for the rest.\n",
        "\n",
        "**Expected Rewards**:\n",
        "- 1.5 - 1.7 on `state_vec_env_1`\n",
        "- 1.5 - 1.6 on `real_vec_env_1`\n",
        "\n",
        "**Expected Success**: Around 0.95 on `real_env`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eK8b9zbqENvd"
      },
      "outputs": [],
      "source": [
        "learner_actor = PPOActor(ckpt=f'{learner_ckpt_path}.zip', environment=state_vec_env_1)\n",
        "\n",
        "# TODO: Evaluate on both environments\n",
        "\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0M_PBcjGVoP"
      },
      "source": [
        "# **[GRAD] Part 4: End-To-End Image-Based Policy**\n",
        "\n",
        "### **4.1: [PROVIDED] Overview**\n",
        "In the previous section we trained the `StateEstimator` and the learner policy separately. First we made a good `StateEstimator` to go from renders to states and only then did we start training a policy to go from states to actions. One common method in practice is to just make a single End-To-End Policy that goes directly from renders to actions.\n",
        "\n",
        "Follow the instructions below to implement each of the components of this end-to-end policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb8kbgdJJQIi"
      },
      "source": [
        "### **4.2: [PROVIDED] Build Custom Gym Environment**\n",
        "\n",
        "The following `EndToEndEnv` class is largely defined for you to train your next PPO policy. In this environment, the reward is calculated the same as `real_env`.\n",
        "\n",
        "This environment is registered with an id of **FetchReachEndToEnd**, which can be initialized using `gym.make` or directly initializing it.\n",
        "\n",
        "**Run the following cell to define and register this environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSPekES7KZHZ"
      },
      "outputs": [],
      "source": [
        "class EndToEndEnv(gym.Env):\n",
        "    def __init__(self, render_mode: str='rgb_array'):\n",
        "        super(EndToEndEnv, self).__init__()\n",
        "        self.metadata = { 'render_modes': ['human', 'rgb_array'], 'render_fps': 30 }\n",
        "        self.render_mode = 'rgb_array'\n",
        "        self.corr_env = real_env\n",
        "\n",
        "        self.transform = T.Compose([T.ToPILImage(), T.ToTensor()])\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(3, 120, 120), dtype=np.float32)\n",
        "        self.action_space = self.corr_env.action_space\n",
        "        self.obs_min = torch.from_numpy(self.observation_space.low).float()\n",
        "        self.obs_max = torch.from_numpy(self.observation_space.high).float()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.corr_env.seed(seed)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        obs, info = self.corr_env.reset()\n",
        "        rgb_array = self.corr_env.render()\n",
        "        return self.transform(rgb_array), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.corr_env.step(action)\n",
        "        rgb_array = self.corr_env.render()\n",
        "        return self.transform(rgb_array), reward, terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        return self.corr_env.render()\n",
        "\n",
        "gym.register(id='FetchReachEndToEnd', entry_point=EndToEndEnv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP_E7dUFKvLV"
      },
      "source": [
        "### **4.3: Define Feature Extractor**\n",
        "\n",
        "In order to configure our end-to-end policy to work with StableBaselines' training, we need to define a `FeatureExtractor` to pass in. This feature extractor will be the exact same as the `StateEstimator` we defined above, except that the final fully connected linear layer will go from `16` to `features_dim`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hpckgcn_K3ko"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "\n",
        "class CustomCNN(BaseFeaturesExtractor):\n",
        "    \"\"\"\n",
        "    A custom CNN feature extractor for image-based observations.\n",
        "    \"\"\"\n",
        "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 128):\n",
        "        # Initialize the base features extractor with the expected output feature dimension\n",
        "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
        "\n",
        "        # TODO: Define architecture\n",
        "        self.conv = None\n",
        "        self.fc1 = None\n",
        "        self.fc2 = None\n",
        "        # END TODO\n",
        "\n",
        "    def forward(self, renders):\n",
        "        # TODO: Calculate the estimated state\n",
        "        return None\n",
        "        # END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6krmMhHOLk_"
      },
      "source": [
        "### **4.3: Train End-To-End Policy**\n",
        "\n",
        "**Instruction 1** Initialize a 3-vectorized and a 1-vectorized `FetchReachEndToEnd` environment. Note that you don't need `env_kwargs` anymore.\n",
        "\n",
        "**Instruction 2** Train a separate PPO policy using `EndToEndEnv`. Similar to before, this model should be trained for 10000 steps, under a 3-vectorized environment, using the same hyperparameters provided in Part 1. Note that because our feature extractor is a CNN, we should make our policy an `CnnPolicy` instead of a `MlpPolicy`.\n",
        "\n",
        "**Note**: Here is a list of created environments after running this following cell:\n",
        "- `real_env`\n",
        "- `real_vec_env_1`\n",
        "- `real_vec_env_3`\n",
        "- `state_vec_env_1`\n",
        "- `state_vec_env_3`\n",
        "- `end_vec_env_1`\n",
        "- `end_vec_env_3`\n",
        "\n",
        "Refer to function documentation for selecting which one to use when doing function calls.\n",
        "\n",
        "**Estimated Training Time**:\n",
        "- 6 - 10 minutes on Google Colab CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kgxyff_kMtT_"
      },
      "outputs": [],
      "source": [
        "endlearner_ckpt_path = 'endlearner'\n",
        "total_steps = 10000\n",
        "reseed(seed)\n",
        "\n",
        "# TODO 1: Create vectorized end-to-end environments\n",
        "end_vec_env_1 = None\n",
        "end_vec_env_3 = None\n",
        "# END TODO\n",
        "\n",
        "end_callback = PPOCallback(save_path=endlearner_ckpt_path, eval_env=end_vec_env_1)\n",
        "policy_kwargs = dict(features_extractor_class=CustomCNN, features_extractor_kwargs=dict(features_dim=128))\n",
        "\n",
        "# TODO 2: Initiate training\n",
        "endlearner = None\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSDKPdfGP1C7"
      },
      "source": [
        "### **4.4: Evaluate PPO On The Custom Environment**\n",
        "\n",
        "Note to save time only use 20 trajectores for `end_vec_env_1` and 100 trajectories for the rest.\n",
        "\n",
        "**Expected Rewards:** About 1.6 to 1.8 on `end_vec_env_1`\n",
        "\n",
        "**Expected Success:** Around 0.95 on `end_env`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TywLWjTP6qk"
      },
      "outputs": [],
      "source": [
        "endlearner_actor = PPOActor(ckpt=f'{endlearner_ckpt_path}.zip', environment=end_vec_env_1)\n",
        "end_env = gym.make('FetchReachEndToEnd')\n",
        "\n",
        "# TODO: Evaluate on custom environment\n",
        "\n",
        "# END TODO"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
